-
  layout: paper
  paper-type: thesis
  selected: y
  year: 2013
  img: thesis
  title: Exact Sampling and Optimisation in Statistical Machine Translation
  authors: Wilker Aziz
  institution: University of Wolverhampton
  doc-url: http://rgcl.wlv.ac.uk/papers/aziz-thesis.pdf
  bibtex: >
    @PhdThesis{aziz-phd,
       author =   {Wilker Aziz},
       title =    {Exact Sampling and Optimisation in Statistical Machine Translation},
       year =     {2013},
       address =  {Wolverhampton, UK},
       URL =      {http://clg.wlv.ac.uk/papers/aziz-thesis.pdf}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2013
  img: wmt2013
  title: Investigations in Exact Inference for Hierarchical Translation
  authors: Wilker Aziz, Marc Dymetman and Sriram Venkatapathy
  booktitle: Proceedings of the 8th Workshop on Statistical Machine Translation
  doc-url: http://www.aclweb.org/anthology/W/W13/W13-2260.pdf
  booktitle-url: http://www.statmt.org/wmt13/
  #code: https://github.com/wilkeraziz/chisel
  slides: /slides/wmt2013.pdf
  venue: conference
  abstract: >
    We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling.
    We also present a first implementation of that method along with experimental results shedding light on some fundamental issues.
    In hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model.
    We replace this intractable distribution by a sequence of tractable upperbounds for which exact optimisers and samplers are easy to obtain.
    Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions.
    While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework.
  bibtex: >
    @InProceedings{aziz-dymetman-venkatapathy:2013:WMT,
      author    = {Aziz, Wilker  and  Dymetman, Marc  and  Venkatapathy, Sriram},
      title     = {Investigations in Exact Inference for Hierarchical Translation},
      booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
      month     = {August},
      year      = {2013},
      address   = {Sofia, Bulgaria},
      publisher = {Association for Computational Linguistics},
      pages     = {472--483},
      url       = {http://www.aclweb.org/anthology/W13-2260}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2013
  img: conll2013
  title: Multilingual WSD-like Constraints for Paraphrase Extraction
  authors: Wilker Aziz and Lucia Specia
  booktitle: Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL)
  doc-url: http://www.aclweb.org/anthology/W/W13/W13-3522.pdf
  booktitle-url: http://www.clips.uantwerpen.be/conll2013/
  #code: https://github.com/wilkeraziz/chisel
  slides: /slides/conll2013.pdf
  poster: /posters/conll2013.pdf
  venue: conference
  abstract: >
    The use of pivot languages and word-alignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases. However, inherent ambiguities in the pivot language(s) can lead to inadequate paraphrases.
    We propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language, i.e., the language to be paraphrased. Text in the input language is annotated with "senses" in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment. This approach shows 62% relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent.
  bibtex: >
    @InProceedings{aziz-specia:2013:CoNLL-2013,
      author    = {Aziz, Wilker  and  Specia, Lucia},
      title     = {Multilingual WSD-like Constraints for Paraphrase Extraction},
      booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
      month     = {August},
      year      = {2013},
      address   = {Sofia, Bulgaria},
      publisher = {Association for Computational Linguistics},
      pages     = {202--211},
      url       = {http://www.aclweb.org/anthology/W13-3522}
    }
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2013
  img: tsd2013
  title: Ranking Machine Translation Systems via Post-Editing
  authors: Wilker Aziz, Ruslan Mitkov and Lucia Specia
  booktitle: Proceedings of Text, Speech and Dialogue (TSD)
  pages: 410-418
  doc-url: http://dx.doi.org/10.1007/978-3-642-40585-3_52
  draft: /papers/tsd2013.pdf
  poster: /posters/tsd2013.pdf
  data: https://surfdrive.surf.nl/files/index.php/s/r650Xek2Cf1t3rN/download
  booktitle-url: http://www.kiv.zcu.cz/tsd2013/
  venue: conference
  abstract: >
    In this paper we investigate ways in which information from the post-editing of machine translations can be used to rank translation systems for quality. In addition to the commonly used edit distance between the raw translation and its edited version, we consider post-editing time and keystroke logging, since these can account not only for technical effort, but also cognitive effort. In this system ranking scenario, post-editing poses some important challenges: i) multiple post-editors are required since having the same annotator fixing alternative translations of a given input segment can bias their post-editing; ii) achieving high enough inter-annotator agreement requires extensive training, which is not always feasible; iii) there exists a natural variation among post-editors, particularly w.r.t. editing time and keystrokes, which makes their measurements less directly comparable. Our experiments involve untrained human annotators, but we propose ways to normalise their post-editing effort indicators to make them comparable. We test these methods using a standard dataset from a machine translation evaluation campaign and show that they yield reliable rankings of systems.
  bibtex: >
    @InProceedings{Aziz+2013:TSD,
      author    = {Aziz, Wilker and Mitkov, Ruslan and  Specia, Lucia},
      title     = {Ranking Machine Translation Systems via Post-Editing},
      booktitle = {In Proceedings of Text, Speech and Dialogue (TSD)},
      series = {Lecture Notes in Computer Science},
      volume = {8082},
      day = {1-5},
      month     = {September},
      year      = {2013},
      address   = {Pilsen, Czech Republic},
      publisher = {Springer Berlin Heidelberg},
      pages     = {410--418},
      url       = {http://link.springer.com/chapter/10.1007/978-3-642-40585-3_52},
      doi = {10.1007/978-3-642-40585-3_52}
    }
